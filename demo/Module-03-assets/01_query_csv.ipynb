{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Module 02 - 01 query multiple files using Spark\r\n",
        "\r\n",
        "Set up variable for later reference. Make sure the name of your datalake is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "datalake = 'datalake3mstaeetovkk4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Just read in some data using CSV format \r\n",
        "\r\n",
        "- Notice we do not have header information in the file\r\n",
        "- Notice we have specified to load in ALL csv files from this folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', format='csv', header=False)\r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Notice that the file does not contain any field names in the header. Let's print the schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's define our own schema on read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql import types\r\n",
        "\r\n",
        "customSchema = types.StructType([\r\n",
        "    types.StructField(\"SalesOrderNumber\", types.StringType(), True),\r\n",
        "    types.StructField(\"SalesTerritoryKey\", types.IntegerType(), True),\r\n",
        "    types.StructField(\"OrderDate\", types.DateType(), True),\r\n",
        "    types.StructField(\"Customer\", types.StringType(), True),\r\n",
        "    types.StructField(\"Email\", types.StringType(), True),\r\n",
        "    types.StructField(\"Adress\", types.StringType(), True),\r\n",
        "    types.StructField(\"Quantity\", types.IntegerType(), True),\r\n",
        "    types.StructField(\"UnitPrice\", types.DoubleType(), True),\r\n",
        "    types.StructField(\"ShippingCost\", types.DoubleType(), True),\r\n",
        "])\r\n",
        "\r\n",
        "df = spark.read \\\r\n",
        "    .csv('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', schema=customSchema)\r\n",
        "\r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now, let's filter this data using Spark SQL. First we register this table as a temporary view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.createOrReplaceTempView(\"sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "And now we can apply the **%%sql** magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "SELECT * \r\n",
        "FROM SALES \r\n",
        "WHERE Customer LIKE 'E%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We can also apply grouping and mathematical operations like SUM. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "SELECT \r\n",
        "    OrderDate,\r\n",
        "    SUM( (Quantity * UnitPrice) + ShippingCost ) AS TotalSales\r\n",
        "FROM Sales\r\n",
        "GROUP BY OrderDate\r\n",
        "ORDER BY OrderDate\r\n",
        "LIMIT 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create a new temporary view by using SQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW TotalSalesByDate\r\n",
        "AS\r\n",
        "\r\n",
        "SELECT \r\n",
        "    OrderDate,\r\n",
        "    SUM( (Quantity * UnitPrice) + ShippingCost ) AS TotalSales\r\n",
        "FROM Sales\r\n",
        "GROUP BY OrderDate\r\n",
        "ORDER BY OrderDate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "... and then use the temporary view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "SELECT * FROM TotalSalesByDate LIMIT 10"
      ]
    }
  ]
}